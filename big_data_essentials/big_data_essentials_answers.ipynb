{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Course- Big Data Essentials\n",
    "\n",
    "## Project Context\n",
    "The H-1B is an employment-based, non-immigrant visa category for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, an US employer must offer a job and petition for H-1B visa with the US immigration department. This is the most common visa status applied for and held by international students once they complete college/ higher education (Masters, PhD) and work in a full-time position.\n",
    "The Office of Foreign Labor Certification (OFLC) generates program data that is useful information about the immigration programs including the H1-B visa. The disclosure data updated annually is available at their official website.\n",
    "\n",
    "### Data Set:\n",
    "The dataset description is as follows: The columns in the dataset include:\n",
    "\n",
    "#### CASE_STATUS: \n",
    "Status associated with the last significant event or decision. Valid values include \"Certified\",\"Certified-Withdrawn\",\"Denied\"and\"Withdrawn\". \n",
    "\n",
    "1. **Certified**: Employer filed the LCA, which was approved by DOL \n",
    "2. **Certified Withdrawn**: LCA was approved but later withdrawn by employer \n",
    "3. **Withdrawn**: LCA was withdrawn by employer before \n",
    "4. **Denied**: LCA was denied by DOL \n",
    "\n",
    "#### **EMPLOYER_NAME**:\n",
    "Name of employer submitting labor condition application. \n",
    "\n",
    "#### **SOC_NAME**:\n",
    "the Occupational name associated with the SOC_CODE. SOC_CODE is the occupational code associated with the job being requested for temporary labor condition, as classified by the Standard Occupational Classification (SOC) System. \n",
    "\n",
    "#### **JOB_TITLE**: Title of the job \n",
    "\n",
    "#### FULL_TIME_POSITION:\n",
    "Y = Full Time Position N = Part Time Position \n",
    "\n",
    "#### PREVAILING_WAGE: \n",
    "Prevailing Wage for the job being requested for temporary labor condition. The wage is listed at annual scale in USD. The prevailing wage for a job position is defined as the average wage paid to similarly employed workers in the requested occupation in the area of intended employment. The prevailing wage is based on the employer’s minimum requirements for the position. \n",
    "\n",
    "#### YEAR: \n",
    "Year in which the H1B visa petition was filed \n",
    "\n",
    "#### WORKSITE: \n",
    "City and State information of the foreign worker’s intended area of employment \n",
    "\n",
    "#### lon:\n",
    "longitude of the Worksite \n",
    "\n",
    "#### lat:\n",
    "latitude of the Worksite\n",
    "\n",
    "### Data Source:\n",
    "File Name Format Size Location h1b_data.csv CSV 470 SharedLocation Note: Please don’t delete the CSV file once you download from the shared location.\n",
    "\n",
    "\n",
    "### Big Data Technologies to be applied:\n",
    "##### HDFS:\n",
    "The input CSV file will be loaded into HDFS residing in respective cloud lab. The output will be stored on HDFS by creating dedicated directories for the same\n",
    "##### Yarn and MapReduce:\n",
    "It’s a processing framework. A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.\n",
    "\n",
    "#### Hive:\n",
    "It’s a processing tool. Hive is a SQL like query language which is often used as the interface to an Apache Hadoop based data warehouse. Hive is considered friendlier and more familiar to users who are used to using SQL for querying data.\n",
    "\n",
    "#### Pig:\n",
    "A scripting platform for processing and analyzing large data sets. Apache Pig allows Apache Hadoop users to write complex MapReduce transformations using a simple scripting language called Pig Latin.\n",
    "\n",
    "#### Hbase:\n",
    "It's a non-relational (NoSQL) database that runs on top of HDFS. HBase is natively integrated with Hadoop and works seamlessly alongside other data access engines through YARN.\n",
    "\n",
    "#### Spark:\n",
    "Apache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets.\n",
    "\n",
    "### Requirements/Use cases/questions\n",
    "\n",
    "1. Is the number of petitions with Data Engineer job title increasing over time? \n",
    "2. Find top 5 job titles who are having highest growth in applications.\n",
    "3. Which part of the US has the most Hardware Engineer jobs for each year? \n",
    "4. Find top 5 locations in the US who have got certified visa for each year. \n",
    "5. Which industry has the most number of Data Scientist positions? \n",
    "6. Which top 5 employers file the most petitions each year? \n",
    "7. Find the most popular top 10 job positions for H1B visa applications for each year? \n",
    "8. Find the percentage and the count of each case status on total applications for each year.\n",
    "\n",
    "9. Find the average Prevailing Wage for each Job for each Year (take part time and full time separate). Arrange the output in descending order. \n",
    "10. Which are employers along with the number of petitions who have the success rate more than 70% in petitions and total petitions filed more than 1000? \n",
    "11. Which are the job positions along with the number of petitions which have the success rate more than 70% in petitions and total petitions filed more than 1000?\n",
    "\n",
    "### Solution expectation:\n",
    "* Step 1: Load datasets to HDFS \n",
    "* Step 2: Write MapReduce program for questions: 1, 2 & 3 \n",
    "* Step 3: Write Hive based queries for questions: 4 & 5 \n",
    "* Step 4: Write Pig scripting for questions: 6 & 7 \n",
    "* Step 5: Write Hbase queries for questions: 8 & 9 \n",
    "* Step 6; Write Spark based queries for question: 10 & 11\n",
    "\n",
    "### Procedure to submit the solution:\n",
    "1. Submit both solution document for each questions along with screen capture of output from your screen. \n",
    "2. Solution document should contain respective program/query/script for the corresponding questions. \n",
    "3. Submit your solution as per guidelines shared by program management team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load datasets to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwx------   - bdhfeb201 bdhfeb201          0 2020-02-27 07:00 /user/bdhfeb201/.Trash\r\n",
      "drwx------   - bdhfeb201 bdhfeb201          0 2020-02-27 10:49 /user/bdhfeb201/.staging\r\n",
      "drwxr-xr-x   - bdhfeb201 bdhfeb201          0 2020-02-26 06:21 /user/bdhfeb201/h-1b-visa\r\n",
      "drwxr-xr-x   - bdhfeb201 bdhfeb201          0 2020-02-27 10:49 /user/bdhfeb201/wordcount\r\n",
      "-rw-r--r--   2 bdhfeb201 bdhfeb201         28 2020-02-27 10:47 /user/bdhfeb201/wordcount_test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/bdhfeb201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hdfs dfs -copyFromLocal h-1b-visa/ /user/bdhfeb201/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwx------   - bdhfeb201 bdhfeb201          0 2020-02-27 07:00 /user/bdhfeb201/.Trash\r\n",
      "drwx------   - bdhfeb201 bdhfeb201          0 2020-02-27 10:49 /user/bdhfeb201/.staging\r\n",
      "drwxr-xr-x   - bdhfeb201 bdhfeb201          0 2020-02-26 06:21 /user/bdhfeb201/h-1b-visa\r\n",
      "drwxr-xr-x   - bdhfeb201 bdhfeb201          0 2020-02-27 10:49 /user/bdhfeb201/wordcount\r\n",
      "-rw-r--r--   2 bdhfeb201 bdhfeb201         28 2020-02-27 10:47 /user/bdhfeb201/wordcount_test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/bdhfeb201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Write MapReduce program for questions: 1, 2 & 3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the number of petitions with Data Engineer job title increasing over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.io.IOException;\r\n",
      "import java.util.StringTokenizer;\r\n",
      "\r\n",
      "import org.apache.hadoop.conf.Configuration;\r\n",
      "import org.apache.hadoop.fs.Path;\r\n",
      "import org.apache.hadoop.io.IntWritable;\r\n",
      "import org.apache.hadoop.io.Text;\r\n",
      "import org.apache.hadoop.mapreduce.Job;\r\n",
      "import org.apache.hadoop.mapreduce.Mapper;\r\n",
      "import org.apache.hadoop.mapreduce.Reducer;\r\n",
      "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n",
      "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n",
      "\r\n",
      "public class Q1DataEngineerIncreasing {\r\n",
      "\r\n",
      "  public static class DataEngineerMapper {\r\n",
      "       extends Mapper<Object, Text, Text, IntWritable>{\r\n",
      "\r\n",
      "    private final static IntWritable one = new IntWritable(1);\r\n",
      "    private Text word = new Text();\r\n",
      "\r\n",
      "    public void map(Object key, Text value, Context context\r\n",
      "                    ) throws IOException, InterruptedException {\r\n",
      "      StringTokenizer itr = new StringTokenizer(value.toString());\r\n",
      "      String[] arrOfStr = value.toString()\r\n",
      "      while (itr.hasMoreTokens()) {\r\n",
      "        word.set(itr.nextToken());\r\n",
      "        context.write(word, one);\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  public static class IntSumReducer\r\n",
      "       extends Reducer<Text,IntWritable,Text,IntWritable> {\r\n",
      "    private IntWritable result = new IntWritable();\r\n",
      "\r\n",
      "    public void reduce(Text key, Iterable<IntWritable> values,\r\n",
      "                       Context context\r\n",
      "                       ) throws IOException, InterruptedException {\r\n",
      "      int sum = 0;\r\n",
      "      for (IntWritable val : values) {\r\n",
      "        sum += val.get();\r\n",
      "      }\r\n",
      "      result.set(sum);\r\n",
      "      context.write(key, result);\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  public static void main(String[] args) throws Exception {\r\n",
      "    Configuration conf = new Configuration();\r\n",
      "    Job job = Job.getInstance(conf, \"Q1DataEngineerIncreasing\");\r\n",
      "    job.setJarByClass(WordCount.class);\r\n",
      "    job.setMapperClass(TokenizerMapper.class);\r\n",
      "    job.setCombinerClass(IntSumReducer.class);\r\n",
      "    job.setReducerClass(IntSumReducer.class);\r\n",
      "    job.setOutputKeyClass(Text.class);\r\n",
      "    job.setOutputValueClass(IntWritable.class);\r\n",
      "    FileInputFormat.addInputPath(job, new Path(args[0]));\r\n",
      "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n",
      "    System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat Q1DataEngineerIncreasing.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find top 5 job titles who are having highest growth in applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Which part of the US has the most Hardware Engineer jobs for each year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write Hive based queries for questions: 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find top 5 locations in the US who have got certified visa for each year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which industry has the most number of Data Scientist positions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write Pig scripting for questions: 6 & 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Which top 5 employers file the most petitions each year? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Find the most popular top 10 job positions for H1B visa applications for each year? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Write Hbase queries for questions: 8 & 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Find the percentage and the count of each case status on total applications for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Find the average Prevailing Wage for each Job for each Year (take part time and full time separate). Arrange the output in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6; Write Spark based queries for question: 10 & 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Which are employers along with the number of petitions who have the success rate more than 70% in petitions and total petitions filed more than 1000? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Which are the job positions along with the number of petitions which have the success rate more than 70% in petitions and total petitions filed more than 1000?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
