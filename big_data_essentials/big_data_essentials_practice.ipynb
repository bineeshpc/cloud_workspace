{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Course- Big Data Essentials\n",
    "\n",
    "## Project Context\n",
    "The H-1B is an employment-based, non-immigrant visa category for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, an US employer must offer a job and petition for H-1B visa with the US immigration department. This is the most common visa status applied for and held by international students once they complete college/ higher education (Masters, PhD) and work in a full-time position.\n",
    "The Office of Foreign Labor Certification (OFLC) generates program data that is useful information about the immigration programs including the H1-B visa. The disclosure data updated annually is available at their official website.\n",
    "\n",
    "### Data Set:\n",
    "The dataset description is as follows: The columns in the dataset include:\n",
    "\n",
    "#### CASE_STATUS: \n",
    "Status associated with the last significant event or decision. Valid values include \"Certified\",\"Certified-Withdrawn\",\"Denied\"and\"Withdrawn\". \n",
    "\n",
    "1. **Certified**: Employer filed the LCA, which was approved by DOL \n",
    "2. **Certified Withdrawn**: LCA was approved but later withdrawn by employer \n",
    "3. **Withdrawn**: LCA was withdrawn by employer before \n",
    "4. **Denied**: LCA was denied by DOL \n",
    "\n",
    "#### **EMPLOYER_NAME**:\n",
    "Name of employer submitting labor condition application. \n",
    "\n",
    "#### **SOC_NAME**:\n",
    "the Occupational name associated with the SOC_CODE. SOC_CODE is the occupational code associated with the job being requested for temporary labor condition, as classified by the Standard Occupational Classification (SOC) System. \n",
    "\n",
    "#### **JOB_TITLE**: Title of the job \n",
    "\n",
    "#### FULL_TIME_POSITION:\n",
    "Y = Full Time Position N = Part Time Position \n",
    "\n",
    "#### PREVAILING_WAGE: \n",
    "Prevailing Wage for the job being requested for temporary labor condition. The wage is listed at annual scale in USD. The prevailing wage for a job position is defined as the average wage paid to similarly employed workers in the requested occupation in the area of intended employment. The prevailing wage is based on the employer’s minimum requirements for the position. \n",
    "\n",
    "#### YEAR: \n",
    "Year in which the H1B visa petition was filed \n",
    "\n",
    "#### WORKSITE: \n",
    "City and State information of the foreign worker’s intended area of employment \n",
    "\n",
    "#### lon:\n",
    "longitude of the Worksite \n",
    "\n",
    "#### lat:\n",
    "latitude of the Worksite\n",
    "\n",
    "### Data Source:\n",
    "File Name Format Size Location h1b_data.csv CSV 470 SharedLocation Note: Please don’t delete the CSV file once you download from the shared location.\n",
    "\n",
    "\n",
    "### Big Data Technologies to be applied:\n",
    "##### HDFS:\n",
    "The input CSV file will be loaded into HDFS residing in respective cloud lab. The output will be stored on HDFS by creating dedicated directories for the same\n",
    "##### Yarn and MapReduce:\n",
    "It’s a processing framework. A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.\n",
    "\n",
    "#### Hive:\n",
    "It’s a processing tool. Hive is a SQL like query language which is often used as the interface to an Apache Hadoop based data warehouse. Hive is considered friendlier and more familiar to users who are used to using SQL for querying data.\n",
    "\n",
    "#### Pig:\n",
    "A scripting platform for processing and analyzing large data sets. Apache Pig allows Apache Hadoop users to write complex MapReduce transformations using a simple scripting language called Pig Latin.\n",
    "\n",
    "#### Hbase:\n",
    "It's a non-relational (NoSQL) database that runs on top of HDFS. HBase is natively integrated with Hadoop and works seamlessly alongside other data access engines through YARN.\n",
    "\n",
    "#### Spark:\n",
    "Apache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets.\n",
    "\n",
    "### Requirements/Use cases/questions\n",
    "\n",
    "1. Is the number of petitions with Data Engineer job title increasing over time? \n",
    "2. Find top 5 job titles who are having highest growth in applications.\n",
    "3. Which part of the US has the most Hardware Engineer jobs for each year? \n",
    "4. Find top 5 locations in the US who have got certified visa for each year. \n",
    "5. Which industry has the most number of Data Scientist positions? \n",
    "6. Which top 5 employers file the most petitions each year? \n",
    "7. Find the most popular top 10 job positions for H1B visa applications for each year? \n",
    "8. Find the percentage and the count of each case status on total applications for each year.\n",
    "\n",
    "9. Find the average Prevailing Wage for each Job for each Year (take part time and full time separate). Arrange the output in descending order. \n",
    "10. Which are employers along with the number of petitions who have the success rate more than 70% in petitions and total petitions filed more than 1000? \n",
    "11. Which are the job positions along with the number of petitions which have the success rate more than 70% in petitions and total petitions filed more than 1000?\n",
    "\n",
    "### Solution expectation:\n",
    "* Step 1: Load datasets to HDFS \n",
    "* Step 2: Write MapReduce program for questions: 1, 2 & 3 \n",
    "* Step 3: Write Hive based queries for questions: 4 & 5 \n",
    "* Step 4: Write Pig scripting for questions: 6 & 7 \n",
    "* Step 5: Write Hbase queries for questions: 8 & 9 \n",
    "* Step 6; Write Spark based queries for question: 10 & 11\n",
    "\n",
    "### Procedure to submit the solution:\n",
    "1. Submit both solution document for each questions along with screen capture of output from your screen. \n",
    "2. Solution document should contain respective program/query/script for the corresponding questions. \n",
    "3. Submit your solution as per guidelines shared by program management team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"h-1b-visa/h1b_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>SOC_NAME</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>FULL_TIME_POSITION</th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WORKSITE</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CERTIFIED-WITHDRAWN</td>\n",
       "      <td>UNIVERSITY OF MICHIGAN</td>\n",
       "      <td>BIOCHEMISTS AND BIOPHYSICISTS</td>\n",
       "      <td>POSTDOCTORAL RESEARCH FELLOW</td>\n",
       "      <td>N</td>\n",
       "      <td>36067.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ANN ARBOR, MICHIGAN</td>\n",
       "      <td>-83.743038</td>\n",
       "      <td>42.280826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CERTIFIED-WITHDRAWN</td>\n",
       "      <td>GOODMAN NETWORKS, INC.</td>\n",
       "      <td>CHIEF EXECUTIVES</td>\n",
       "      <td>CHIEF OPERATING OFFICER</td>\n",
       "      <td>Y</td>\n",
       "      <td>242674.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>PLANO, TEXAS</td>\n",
       "      <td>-96.698886</td>\n",
       "      <td>33.019843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CERTIFIED-WITHDRAWN</td>\n",
       "      <td>PORTS AMERICA GROUP, INC.</td>\n",
       "      <td>CHIEF EXECUTIVES</td>\n",
       "      <td>CHIEF PROCESS OFFICER</td>\n",
       "      <td>Y</td>\n",
       "      <td>193066.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>JERSEY CITY, NEW JERSEY</td>\n",
       "      <td>-74.077642</td>\n",
       "      <td>40.728158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CERTIFIED-WITHDRAWN</td>\n",
       "      <td>GATES CORPORATION, A WHOLLY-OWNED SUBSIDIARY O...</td>\n",
       "      <td>CHIEF EXECUTIVES</td>\n",
       "      <td>REGIONAL PRESIDEN, AMERICAS</td>\n",
       "      <td>Y</td>\n",
       "      <td>220314.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>DENVER, COLORADO</td>\n",
       "      <td>-104.990251</td>\n",
       "      <td>39.739236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>WITHDRAWN</td>\n",
       "      <td>PEABODY INVESTMENTS CORP.</td>\n",
       "      <td>CHIEF EXECUTIVES</td>\n",
       "      <td>PRESIDENT MONGOLIA AND INDIA</td>\n",
       "      <td>Y</td>\n",
       "      <td>157518.4</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ST. LOUIS, MISSOURI</td>\n",
       "      <td>-90.199404</td>\n",
       "      <td>38.627003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CASE_STATUS                                      EMPLOYER_NAME  \\\n",
       "1  CERTIFIED-WITHDRAWN                             UNIVERSITY OF MICHIGAN   \n",
       "2  CERTIFIED-WITHDRAWN                             GOODMAN NETWORKS, INC.   \n",
       "3  CERTIFIED-WITHDRAWN                          PORTS AMERICA GROUP, INC.   \n",
       "4  CERTIFIED-WITHDRAWN  GATES CORPORATION, A WHOLLY-OWNED SUBSIDIARY O...   \n",
       "5            WITHDRAWN                          PEABODY INVESTMENTS CORP.   \n",
       "\n",
       "                        SOC_NAME                     JOB_TITLE  \\\n",
       "1  BIOCHEMISTS AND BIOPHYSICISTS  POSTDOCTORAL RESEARCH FELLOW   \n",
       "2               CHIEF EXECUTIVES       CHIEF OPERATING OFFICER   \n",
       "3               CHIEF EXECUTIVES         CHIEF PROCESS OFFICER   \n",
       "4               CHIEF EXECUTIVES   REGIONAL PRESIDEN, AMERICAS   \n",
       "5               CHIEF EXECUTIVES  PRESIDENT MONGOLIA AND INDIA   \n",
       "\n",
       "  FULL_TIME_POSITION  PREVAILING_WAGE    YEAR                 WORKSITE  \\\n",
       "1                  N          36067.0  2016.0      ANN ARBOR, MICHIGAN   \n",
       "2                  Y         242674.0  2016.0             PLANO, TEXAS   \n",
       "3                  Y         193066.0  2016.0  JERSEY CITY, NEW JERSEY   \n",
       "4                  Y         220314.0  2016.0         DENVER, COLORADO   \n",
       "5                  Y         157518.4  2016.0      ST. LOUIS, MISSOURI   \n",
       "\n",
       "          lon        lat  \n",
       "1  -83.743038  42.280826  \n",
       "2  -96.698886  33.019843  \n",
       "3  -74.077642  40.728158  \n",
       "4 -104.990251  39.739236  \n",
       "5  -90.199404  38.627003  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CASE_STATUS', 'EMPLOYER_NAME', 'SOC_NAME', 'JOB_TITLE',\n",
       "       'FULL_TIME_POSITION', 'PREVAILING_WAGE', 'YEAR', 'WORKSITE', 'lon',\n",
       "       'lat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class HelloWorld \r\n",
      "{ \r\n",
      "    // Your program begins with a call to main(). \r\n",
      "    // Prints \"Hello, World\" to the terminal window. \r\n",
      "    public static void main(String args[]) \r\n",
      "    { \r\n",
      "        System.out.println(\"Hello, World\"); \r\n",
      "    } \r\n",
      "} "
     ]
    }
   ],
   "source": [
    "!cat HelloWorld.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World\r\n"
     ]
    }
   ],
   "source": [
    "!javac HelloWorld.java\n",
    "!java HelloWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "required configuration\n",
    "export HADOOP_CLASSPATH=/usr/lib/jvm/java-7-openjdk-amd64/lib/tools.jar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env HADOOP_CLASSPATH /usr/lib/jvm/java-7-openjdk-amd64/lib/tools.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.io.IOException;\r\n",
      "import java.util.StringTokenizer;\r\n",
      "\r\n",
      "import org.apache.hadoop.conf.Configuration;\r\n",
      "import org.apache.hadoop.fs.Path;\r\n",
      "import org.apache.hadoop.io.IntWritable;\r\n",
      "import org.apache.hadoop.io.Text;\r\n",
      "import org.apache.hadoop.mapreduce.Job;\r\n",
      "import org.apache.hadoop.mapreduce.Mapper;\r\n",
      "import org.apache.hadoop.mapreduce.Reducer;\r\n",
      "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n",
      "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n",
      "\r\n",
      "public class WordCount {\r\n",
      "\r\n",
      "  public static class TokenizerMapper\r\n",
      "       extends Mapper<Object, Text, Text, IntWritable>{\r\n",
      "\r\n",
      "    private final static IntWritable one = new IntWritable(1);\r\n",
      "    private Text word = new Text();\r\n",
      "\r\n",
      "    public void map(Object key, Text value, Context context\r\n",
      "                    ) throws IOException, InterruptedException {\r\n",
      "      StringTokenizer itr = new StringTokenizer(value.toString());\r\n",
      "      while (itr.hasMoreTokens()) {\r\n",
      "        word.set(itr.nextToken());\r\n",
      "        context.write(word, one);\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  public static class IntSumReducer\r\n",
      "       extends Reducer<Text,IntWritable,Text,IntWritable> {\r\n",
      "    private IntWritable result = new IntWritable();\r\n",
      "\r\n",
      "    public void reduce(Text key, Iterable<IntWritable> values,\r\n",
      "                       Context context\r\n",
      "                       ) throws IOException, InterruptedException {\r\n",
      "      int sum = 0;\r\n",
      "      for (IntWritable val : values) {\r\n",
      "        sum += val.get();\r\n",
      "      }\r\n",
      "      result.set(sum);\r\n",
      "      context.write(key, result);\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  public static void main(String[] args) throws Exception {\r\n",
      "    Configuration conf = new Configuration();\r\n",
      "    Job job = Job.getInstance(conf, \"word count\");\r\n",
      "    job.setJarByClass(WordCount.class);\r\n",
      "    job.setMapperClass(TokenizerMapper.class);\r\n",
      "    job.setCombinerClass(IntSumReducer.class);\r\n",
      "    job.setReducerClass(IntSumReducer.class);\r\n",
      "    job.setOutputKeyClass(Text.class);\r\n",
      "    job.setOutputValueClass(IntWritable.class);\r\n",
      "    FileInputFormat.addInputPath(job, new Path(args[0]));\r\n",
      "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n",
      "    System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat WordCount.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Hadoop Goodbye Hadoop\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop com.sun.tools.javac.Main WordCount.java\n",
    "!jar cf wc.jar WordCount*.class\n",
    "!echo \"Hello Hadoop Goodbye Hadoop\" > wordcount_test.txt\n",
    "#!hdfs dfs -copyFromLocal wordcount_test.txt /user/bdhfeb201/\n",
    "!hdfs dfs -cat /user/bdhfeb201/wordcount_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/02 04:40:12 INFO client.RMProxy: Connecting to ResourceManager at ip-10-0-1-20.ec2.internal/10.0.1.20:8032\n",
      "20/03/02 04:40:12 WARN security.UserGroupInformation: PriviledgedActionException as:bdhfeb201 (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://ip-10-0-1-20.ec2.internal:8020/user/bdhfeb201/wordcount/output already exists\n",
      "Exception in thread \"main\" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://ip-10-0-1-20.ec2.internal:8020/user/bdhfeb201/wordcount/output already exists\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:270)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)\n",
      "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325)\n",
      "\tat WordCount.main(WordCount.java:59)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar wc.jar WordCount /user/bdhfeb201/wordcount_test.txt /user/bdhfeb201/wordcount/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye\t1\r\n",
      "Hadoop\t2\r\n",
      "Hello\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/bdhfeb201/wordcount/output/part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data[['JOB_TITLE', 'YEAR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d1['JOB_TITLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2= d2.index.str.contains('data eng', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATA ENGINEER                                  591\n",
       "DATA ENGINEER II                               201\n",
       "SENIOR DATA ENGINEER                           153\n",
       "BIG DATA ENGINEER                              143\n",
       "DATA ENGINEER I                                 89\n",
       "                                              ... \n",
       "BIG DATA ENGINEER/SYSTEMS DEVELOPER              1\n",
       "SENIOR SOFTWARE ENGINEER - DATA ENGINEERING      1\n",
       "SOFTWARE ENGINER (DATA ENGINEER)                 1\n",
       "LEAD DATA ENGINEER-ETL                           1\n",
       "BACKEND DATA ENGINEER                            1\n",
       "Name: JOB_TITLE, Length: 217, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.loc[i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat StringSplit.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineer\r\n",
      "manager\r\n",
      "data scientist\r\n",
      "analyste\r\n"
     ]
    }
   ],
   "source": [
    "!javac StringSplit.java\n",
    "!java StringSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "drwx------   - bdhfeb201 bdhfeb201          0 2020-03-02 05:00 /user/bdhfeb201/.Trash\r\n",
      "drwx------   - bdhfeb201 bdhfeb201          0 2020-03-02 04:45 /user/bdhfeb201/.staging\r\n",
      "drwxr-xr-x   - bdhfeb201 bdhfeb201          0 2020-02-26 06:21 /user/bdhfeb201/h-1b-visa\r\n",
      "drwxr-xr-x   - bdhfeb201 bdhfeb201          0 2020-03-02 09:59 /user/bdhfeb201/h-1b-visa-100\r\n",
      "drwxr-xr-x   - bdhfeb201 bdhfeb201          0 2020-03-02 04:45 /user/bdhfeb201/wordcount\r\n",
      "-rw-r--r--   2 bdhfeb201 bdhfeb201         28 2020-02-27 10:47 /user/bdhfeb201/wordcount_test.txt\r\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -copyFromLocal h-1b-visa-100/ /user/bdhfeb201/\n",
    "!hdfs dfs -ls /user/bdhfeb201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.io.IOException;\r\n",
      "import java.util.StringTokenizer;\r\n",
      "\r\n",
      "import org.apache.hadoop.conf.Configuration;\r\n",
      "import org.apache.hadoop.fs.Path;\r\n",
      "import org.apache.hadoop.io.IntWritable;\r\n",
      "import org.apache.hadoop.io.Text;\r\n",
      "import org.apache.hadoop.mapreduce.Job;\r\n",
      "import org.apache.hadoop.mapreduce.Mapper;\r\n",
      "import org.apache.hadoop.mapreduce.Reducer;\r\n",
      "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n",
      "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n",
      "\r\n",
      "public class Q1DataEngineerIncreasing {\r\n",
      "\r\n",
      "  public static class DataEngineerMapper\r\n",
      "       extends Mapper<Object, Text, Text, IntWritable>{\r\n",
      "\r\n",
      "    private final static IntWritable one = new IntWritable(1);\r\n",
      "    private Text word = new Text();\r\n",
      "    private String jobTitle;\r\n",
      "    private final String titleOfInterest = \"CHIEF EXECUTIVES\";\r\n",
      "\r\n",
      "    public void map(Object key, Text value, Context context\r\n",
      "                    ) throws IOException, InterruptedException {\r\n",
      "      String[] arrOfStr = value.toString().split(\",\");\r\n",
      "      jobTitle = arrOfStr[4].trim();\r\n",
      "      if (jobTitle.equals(titleOfInterest)) {    \r\n",
      "          word.set(arrOfStr[7].trim());\r\n",
      "          context.write(word, one);\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  public static class IntSumReducer\r\n",
      "       extends Reducer<Text,IntWritable,Text,IntWritable> {\r\n",
      "    private IntWritable result = new IntWritable();\r\n",
      "\r\n",
      "    public void reduce(Text key, Iterable<IntWritable> values,\r\n",
      "                       Context context\r\n",
      "                       ) throws IOException, InterruptedException {\r\n",
      "      int sum = 0;\r\n",
      "      for (IntWritable val : values) {\r\n",
      "        sum += val.get();\r\n",
      "      }\r\n",
      "      result.set(sum);\r\n",
      "      context.write(key, result);\r\n",
      "    }\r\n",
      "  }\r\n",
      "\r\n",
      "  public static void main(String[] args) throws Exception {\r\n",
      "    Configuration conf = new Configuration();\r\n",
      "    Job job = Job.getInstance(conf, \"Q1DataEngineerIncreasing\");\r\n",
      "    job.setJarByClass(Q1DataEngineerIncreasing.class);\r\n",
      "    job.setMapperClass(DataEngineerMapper.class);\r\n",
      "    job.setCombinerClass(IntSumReducer.class);\r\n",
      "    job.setReducerClass(IntSumReducer.class);\r\n",
      "    job.setOutputKeyClass(Text.class);\r\n",
      "    job.setOutputValueClass(IntWritable.class);\r\n",
      "    FileInputFormat.addInputPath(job, new Path(args[0]));\r\n",
      "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n",
      "    System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat Q1DataEngineerIncreasing.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/02 12:13:36 INFO fs.TrashPolicyDefault: Moved: 'hdfs://ip-10-0-1-20.ec2.internal:8020/user/bdhfeb201/Q1DataEngineerIncreasing' to trash at: hdfs://ip-10-0-1-20.ec2.internal:8020/user/bdhfeb201/.Trash/Current/user/bdhfeb201/Q1DataEngineerIncreasing\n",
      "20/03/02 12:13:38 INFO client.RMProxy: Connecting to ResourceManager at ip-10-0-1-20.ec2.internal/10.0.1.20:8032\n",
      "20/03/02 12:13:38 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "20/03/02 12:13:38 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "20/03/02 12:13:38 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "20/03/02 12:13:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579257682689_8558\n",
      "20/03/02 12:13:39 INFO impl.YarnClientImpl: Submitted application application_1579257682689_8558\n",
      "20/03/02 12:13:39 INFO mapreduce.Job: The url to track the job: http://ip-10-0-1-20.ec2.internal:6088/proxy/application_1579257682689_8558/\n",
      "20/03/02 12:13:39 INFO mapreduce.Job: Running job: job_1579257682689_8558\n",
      "20/03/02 12:13:44 INFO mapreduce.Job: Job job_1579257682689_8558 running in uber mode : false\n",
      "20/03/02 12:13:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/03/02 12:13:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/03/02 12:13:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/03/02 12:13:53 INFO mapreduce.Job: Job job_1579257682689_8558 completed successfully\n",
      "20/03/02 12:13:53 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20\n",
      "\t\tFILE: Number of bytes written=297645\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=15509\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2159\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1674\n",
      "\t\tTotal time spent by all map tasks (ms)=2159\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1674\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2159\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1674\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4421632\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3428352\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=16\n",
      "\t\tInput split bytes=148\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=16\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=93\n",
      "\t\tCPU time spent (ms)=1090\n",
      "\t\tPhysical memory (bytes) snapshot=764411904\n",
      "\t\tVirtual memory (bytes) snapshot=5631426560\n",
      "\t\tTotal committed heap usage (bytes)=1067974656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=15361\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n"
     ]
    }
   ],
   "source": [
    "!hadoop com.sun.tools.javac.Main Q1DataEngineerIncreasing.java\n",
    "!jar cf Q1DataEngineerIncreasing.jar Q1DataEngineerIncreasing*.class\n",
    "!hdfs dfs -rm -r /user/bdhfeb201/Q1DataEngineerIncreasing\n",
    "!hadoop jar Q1DataEngineerIncreasing.jar Q1DataEngineerIncreasing /user/bdhfeb201/h-1b-visa-100 /user/bdhfeb201/Q1DataEngineerIncreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/bdhfeb201/Q1DataEngineerIncreasing/part* | head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
